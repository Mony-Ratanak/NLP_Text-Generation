{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP mini-project Group3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> * Library Installation\n",
    "<br>pip install wikipedia-api\n",
    "<br>pip install nltk\n",
    "<br>nltk.download('punkt')  # For sentence tokenization\n",
    "<br>nltk.download('stopwords')  # For removing stop words\n",
    "<br>nltk.download('wordnet')  # For lexical semantics\n",
    "<br>nltk.download('punkt') #For sentence tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 'Khmer Empire' has been saved to ./datasets\\wiki_data1.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wikipediaapi\n",
    "\n",
    "# Set up the Wikipedia API with a valid user-agent\n",
    "wiki = wikipediaapi.Wikipedia(\n",
    "    language=\"en\", \n",
    "    user_agent=\"YourAppName/1.0 (your-email@example.com)\"\n",
    ")\n",
    "\n",
    "# Specify the topic and folder path\n",
    "topic = \"Khmer Empire\"\n",
    "output_folder = \"./datasets\"\n",
    "output_file = os.path.join(output_folder, \"wiki_data1.txt\")\n",
    "\n",
    "# Fetch the article\n",
    "article = wiki.page(topic)\n",
    "\n",
    "if article.exists():\n",
    "    # Ensure the folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Write the article text to a file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(article.text)\n",
    "    print(f\"Article '{topic}' has been saved to {output_file}\")\n",
    "else:\n",
    "    print(f\"The article '{topic}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8871238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Remove citations like [1], [citation needed]\n",
    "    text = re.sub(r'\\[[^\\]]+\\]', '', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "    # Remove parenthetical content\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "    # Allow commas and periods while removing other punctuation\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,]', '', text)\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Ensure proper sentence spacing (e.g., \"word.word\" -> \"word. word\")\n",
    "    text = re.sub(r'\\.([a-zA-Z])', r'. \\1', text)\n",
    "    return text.lower()  # Convert to lowercase\n",
    "\n",
    "# Load the raw corpus\n",
    "# Load the raw corpus\n",
    "with open('./datasets/wiki_data.txt', 'r', encoding='utf-8') as file:\n",
    "    raw_corpus = file.read()\n",
    "\n",
    "\n",
    "# Clean the corpus\n",
    "cleaned_corpus = clean_text(raw_corpus)\n",
    "\n",
    "# Save the cleaned corpus for inspection (optional)\n",
    "with open('./datasets/cleaned_wiki_data.txt', 'w') as file:\n",
    "    file.write(cleaned_corpus)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Split the Corpus into training (70%), validation (10%) and testing (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 356\n",
      "Training set: 249\n",
      "Validation set: 35\n",
      "Testing set: 72\n",
      "Datasets saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "# Split the text into sentences\n",
    "sentences = cleaned_corpus.split('.')\n",
    "# Remove empty sentences and trim whitespace\n",
    "sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "# Shuffle the sentences to avoid bias\n",
    "random.shuffle(sentences)\n",
    "\n",
    "# Calculate split indices\n",
    "train_split = int(0.7 * len(sentences))\n",
    "val_split = int(0.8 * len(sentences))\n",
    "\n",
    "# Create subsets\n",
    "train_set = sentences[:train_split]\n",
    "val_set = sentences[train_split:val_split]\n",
    "test_set = sentences[val_split:]\n",
    "\n",
    "# Print information about the splits\n",
    "print(f\"Total sentences: {len(sentences)}\")\n",
    "print(f\"Training set: {len(train_set)}\")\n",
    "print(f\"Validation set: {len(val_set)}\")\n",
    "print(f\"Testing set: {len(test_set)}\")\n",
    "\n",
    "# Save the split datasets\n",
    "train_file = './datasets/train_set.txt'\n",
    "val_file = './datasets/val_set.txt'\n",
    "test_file = './datasets/test_set.txt'\n",
    "\n",
    "# Save the data to files\n",
    "with open(train_file, 'w') as f:\n",
    "    f.write('\\n'.join(train_set))\n",
    "\n",
    "with open(val_file, 'w') as f:\n",
    "    f.write('\\n'.join(val_set))\n",
    "\n",
    "with open(test_file, 'w') as f:\n",
    "    f.write('\\n'.join(test_set))\n",
    "\n",
    "print(\"Datasets saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 .Tokenize Sentences and Limit Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokenized sentences in train_set: 249\n",
      "Vocabulary size (top 20000 words): 1542\n",
      "Number of sentences in train_tokens after replacement: 249\n",
      "First 5 tokenized training sentences with <UNK>:\n",
      "['i']\n",
      "['ancient', 'angkor']\n",
      "['his', 'fatherinlaw', ',', 'the', 'king', 'of', 'cambodia', ',', 'gave', 'him', 'a', 'khmer', 'army', 'to', 'create', 'a', 'buffer', 'state', 'in', 'what', 'is', 'now', 'laos']\n",
      "['keyes', ',', 'charles', 'f']\n",
      "['while', 'previously', 'three', 'rice', 'harvests', 'per', 'year', 'were', 'possible', 'a', 'substantial', 'contribution', 'to', 'the', 'prosperity', 'and', 'power', 'of', 'kambuja', 'the', 'declining', 'harvests', 'further', 'weakened', 'the', 'empire']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# Step 1: Tokenize each sentence in the training set\n",
    "train_tokens = [word_tokenize(sentence) for sentence in train_set]\n",
    "\n",
    "# Check if tokenization is successful\n",
    "if len(train_tokens) == 0:\n",
    "    print(\"Error: `train_tokens` is empty. Check `train_set`.\")\n",
    "else:\n",
    "    print(f\"Number of tokenized sentences in train_set: {len(train_tokens)}\")\n",
    "\n",
    "# Step 2: Define the vocabulary size\n",
    "vocab_size = 20000\n",
    "\n",
    "# Step 3: Count word frequencies in the training set\n",
    "word_counts = Counter(word for sentence in train_tokens for word in sentence)\n",
    "\n",
    "# Step 4: Select the top `vocab_size` words as the vocabulary\n",
    "vocab = {word for word, _ in word_counts.most_common(vocab_size)}\n",
    "print(f\"Vocabulary size (top {vocab_size} words): {len(vocab)}\")\n",
    "\n",
    "# Step 5: Function to replace words not in the vocabulary with <UNK>\n",
    "def replace_with_unk(sentences, vocab):\n",
    "    return [\n",
    "        [word if word in vocab else '<UNK>' for word in word_tokenize(sentence)]\n",
    "        for sentence in sentences\n",
    "    ]\n",
    "\n",
    "# Step 6: Apply the function to train, validation, and test sets\n",
    "train_tokens = replace_with_unk(train_set, vocab)\n",
    "val_tokens = replace_with_unk(val_set, vocab)\n",
    "test_tokens = replace_with_unk(test_set, vocab)\n",
    "\n",
    "# Check the length of `train_tokens` after replacement\n",
    "if len(train_tokens) == 0:\n",
    "    print(\"Error: `train_tokens` is empty after replacement. Check `replace_with_unk` function.\")\n",
    "else:\n",
    "    print(f\"Number of sentences in train_tokens after replacement: {len(train_tokens)}\")\n",
    "\n",
    "# Step 7: Print the first 5 tokenized sentences to check results\n",
    "print(\"First 5 tokenized training sentences with <UNK>:\")\n",
    "for i in range(min(5, len(train_tokens))):  # Handle cases where there are fewer than 5 sentences\n",
    "    print(train_tokens[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Build 4-Gram Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-gram model (Backoff) has 4596 entries.\n",
      "4-gram model (Interpolation) has 4596 entries.\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Function to build an n-gram model\n",
    "def build_ngram_model(tokenized_sentences, n=4):\n",
    "    ngram_counts = defaultdict(int)\n",
    "    for sentence in tokenized_sentences:\n",
    "        # padded_sentence = ['<s>'] * (n - 1) + sentence + ['</s>']\n",
    "        padded_sentence = ['<s>'] * (n - 1) + sentence \n",
    "        for i in range(len(padded_sentence) - n + 1):\n",
    "            ngram = tuple(padded_sentence[i:i + n])\n",
    "            ngram_counts[ngram] += 1\n",
    "    return ngram_counts\n",
    "\n",
    "# Build the n-gram models\n",
    "lm1 = build_ngram_model(train_tokens, n=4)  # Backoff model\n",
    "lm2 = build_ngram_model(train_tokens, n=4)  # Interpolation model\n",
    "\n",
    "print(\"4-gram model (Backoff) has\", len(lm1), \"entries.\")\n",
    "print(\"4-gram model (Interpolation) has\", len(lm2), \"entries.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Build model for backoff probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backoff_prob(model, ngram):\n",
    "    \"\"\"\n",
    "    Calculate the probability of an n-gram using a backoff model.\n",
    "    :param model: N-gram counts.\n",
    "    :param ngram: The n-gram tuple.\n",
    "    :return: Probability of the n-gram.\n",
    "    \"\"\"\n",
    "    for i in range(len(ngram), 0, -1):  # Back off through lower-order n-grams\n",
    "        sub_ngram = ngram[-i:]\n",
    "        if sub_ngram in model:\n",
    "            return model[sub_ngram] / sum(\n",
    "                count for ngram_key, count in model.items() if ngram_key[:-1] == sub_ngram[:-1]\n",
    "            )\n",
    "    return 1e-6  # Small probability for unseen n-grams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Build model for interpolation probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolated_prob(model, ngram, lambdas, k=1):\n",
    "\n",
    "    prob = 0.0\n",
    "    for i in range(1, len(ngram) + 1):  # Iterate over all sub-ngrams\n",
    "        sub_ngram = ngram[-i:]\n",
    "        sub_ngram_count = model.get(sub_ngram, 0)\n",
    "        context_count = sum(\n",
    "            count for ngram_key, count in model.items() if ngram_key[:-1] == sub_ngram[:-1]\n",
    "        )\n",
    "        prob += lambdas[i - 1] * ((sub_ngram_count + k) / (context_count + k))\n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Calulation Perplexity for each models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of LM1 (Backoff): 456526.0963889846\n",
      "Perplexity of LM2 (Interpolation): 1.02510106989401\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def calculate_perplexity(model, tokens, prob_func, n=4, lambdas=None, k=1):\n",
    "    log_prob_sum = 0\n",
    "    word_count = 0\n",
    "\n",
    "    for sentence in tokens:\n",
    "        # padded_sentence = [\"<s>\"] * (n - 1) + sentence + [\"</s>\"]\n",
    "        padded_sentence = [\"<s>\"] * (n - 1) + sentence\n",
    "        for i in range(len(padded_sentence) - n + 1):\n",
    "            ngram = tuple(padded_sentence[i:i + n])\n",
    "            prob = prob_func(model, ngram, lambdas, k) if lambdas else prob_func(model, ngram)\n",
    "            log_prob_sum += math.log2(prob)\n",
    "            word_count += 1\n",
    "\n",
    "    return 2 ** (-log_prob_sum / word_count)\n",
    "\n",
    "# Hyperparameters for interpolation\n",
    "lambdas = [0.1, 0.3, 0.4, 0.2]\n",
    "k = 1\n",
    "\n",
    "# Calculate perplexity for Backoff and Interpolation models\n",
    "perplexity_lm1 = calculate_perplexity(lm1, test_tokens, prob_func=backoff_prob, n=4)\n",
    "perplexity_lm2 = calculate_perplexity(lm2, test_tokens, prob_func=interpolated_prob, n=4, lambdas=lambdas, k=k)\n",
    "\n",
    "print(f\"Perplexity of LM1 (Backoff): {perplexity_lm1}\")\n",
    "print(f\"Perplexity of LM2 (Interpolation): {perplexity_lm2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text by LM1 (Backoff): benjamin walker , angkor empire a history of cambodia , an important insight into the khmer empires daily life , market scenes , military marches , and palace life reports\n",
      "Generated Text by LM2 (Interpolation): then come the palace women carrying lances and shields , with the kings private guards\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_text(model, n=4, length=20, prob_func=None, lambdas=None, k=1):\n",
    "    text = [\"<s>\"] * (n - 1)  # Start with padding symbols\n",
    "    for _ in range(length):\n",
    "        context = tuple(text[-(n - 1):])\n",
    "        candidates = [ngram for ngram in model if ngram[:-1] == context]\n",
    "        if candidates:\n",
    "            if lambdas:\n",
    "                weights = [prob_func(model, ngram, lambdas, k) for ngram in candidates]\n",
    "            else:\n",
    "                weights = [prob_func(model, ngram) for ngram in candidates]\n",
    "            chosen = random.choices(candidates, weights=weights)[0]\n",
    "            text.append(chosen[-1])\n",
    "\n",
    "\n",
    "    return \" \".join(text[n - 1:])  # Skip padding symbols in output\n",
    "\n",
    "# Generate text with both models\n",
    "generated_text_lm1 = generate_text(lm1, prob_func=backoff_prob, n=4, length=30)\n",
    "generated_text_lm2 = generate_text(lm2, prob_func=interpolated_prob, n=4, length=30, lambdas=lambdas, k=k)\n",
    "\n",
    "print(\"Generated Text by LM1 (Backoff):\", generated_text_lm1)\n",
    "print(\"Generated Text by LM2 (Interpolation):\", generated_text_lm2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
