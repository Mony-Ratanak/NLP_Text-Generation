{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Cleaning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c8871238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Remove citations like [1], [citation needed]\n",
    "    text = re.sub(r'\\[[^\\]]+\\]', '', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "    # Remove parenthetical content\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "    # Allow commas and periods while removing other punctuation\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,]', '', text)\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Ensure proper sentence spacing (e.g., \"word.word\" -> \"word. word\")\n",
    "    text = re.sub(r'\\.([a-zA-Z])', r'. \\1', text)\n",
    "    return text.lower()  # Convert to lowercase\n",
    "\n",
    "# Load the raw corpus\n",
    "with open('./datasets/angkorwat.txt', 'r') as file:\n",
    "    raw_corpus = file.read()\n",
    "\n",
    "# Clean the corpus\n",
    "cleaned_corpus = clean_text(raw_corpus)\n",
    "\n",
    "# Save the cleaned corpus for inspection (optional)\n",
    "with open('./datasets/cleaned_angkorwat.txt', 'w') as file:\n",
    "    file.write(cleaned_corpus)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Split the Corpus into training (70%), validation (10%) and testing (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 71\n",
      "Training set: 49\n",
      "Validation set: 7\n",
      "Testing set: 15\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Split the text into sentences\n",
    "sentences = cleaned_corpus.split('.')\n",
    "# Remove empty sentences and trim whitespace\n",
    "sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "# Shuffle the sentences to avoid bias\n",
    "random.shuffle(sentences)\n",
    "\n",
    "# Calculate split indices\n",
    "train_split = int(0.7 * len(sentences))\n",
    "val_split = int(0.8 * len(sentences))\n",
    "\n",
    "# Create subsets\n",
    "train_set = sentences[:train_split]\n",
    "val_set = sentences[train_split:val_split]\n",
    "test_set = sentences[val_split:]\n",
    "\n",
    "# Print information about the splits\n",
    "print(f\"Total sentences: {len(sentences)}\")\n",
    "print(f\"Training set: {len(train_set)}\")\n",
    "print(f\"Validation set: {len(val_set)}\")\n",
    "print(f\"Testing set: {len(test_set)}\")\n",
    "\n",
    "\n",
    "# Step 5: Save the split datasets\n",
    "train_file = './datasets/train_set.txt'\n",
    "val_file = './datasets/val_set.txt'\n",
    "test_file = './datasets/test_set.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Tokenize Sentences and Limit Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokenized sentences in train_set: 49\n",
      "Vocabulary size (top 20000 words): 578\n",
      "Number of sentences in train_tokens after replacement: 49\n",
      "First 5 tokenized training sentences with <UNK>:\n",
      "['at', 'its', 'peak', ',', 'the', 'city', 'occupied', 'an', 'area', 'greater', 'than', 'modern', 'paris', ',', 'and', 'its', 'buildings', 'use', 'far', 'more', 'stone', 'than', 'all', 'of', 'the', 'egyptian', 'structures', 'combined']\n",
      "['the', 'name', 'angkor', 'is', 'derived', 'from', 'nokor', ',', 'a', 'khmer', 'word', 'meaning', 'kingdom', 'which', 'in', 'turn', 'derived', 'from', 'sanskrit', 'nagara', ',', 'meaning', 'city']\n",
      "['its', 'neighbors', 'to', 'the', 'east', ',', 'the', 'cham', 'of', 'what', 'is', 'now', 'southern', 'vietnam', ',', 'took', 'advantage', 'of', 'the', 'situation', 'in', '1177', 'to', 'launch', 'a', 'waterborne', 'invasion', 'up', 'the', 'mekong', 'river', 'and', 'across', 'tonl', 'sap']\n",
      "['he', 'also', 'established', 'the', 'city', 'of', 'hariharalaya', 'at', 'the', 'northern', 'end', 'of', 'tonl', 'sap']\n",
      "['the', 'measurements', 'themselves', 'of', 'the', 'temple', 'and', 'its', 'parts', 'in', 'relation', 'to', 'one', 'another', 'have', 'cosmological', 'significance']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# Step 1: Tokenize each sentence in the training set\n",
    "train_tokens = [word_tokenize(sentence) for sentence in train_set]\n",
    "\n",
    "# Check if tokenization is successful\n",
    "if len(train_tokens) == 0:\n",
    "    print(\"Error: `train_tokens` is empty. Check `train_set`.\")\n",
    "else:\n",
    "    print(f\"Number of tokenized sentences in train_set: {len(train_tokens)}\")\n",
    "\n",
    "# Step 2: Define the vocabulary size\n",
    "vocab_size = 20000\n",
    "\n",
    "# Step 3: Count word frequencies in the training set\n",
    "word_counts = Counter(word for sentence in train_tokens for word in sentence)\n",
    "\n",
    "# Step 4: Select the top `vocab_size` words as the vocabulary\n",
    "vocab = {word for word, _ in word_counts.most_common(vocab_size)}\n",
    "print(f\"Vocabulary size (top {vocab_size} words): {len(vocab)}\")\n",
    "\n",
    "# Step 5: Function to replace words not in the vocabulary with <UNK>\n",
    "def replace_with_unk(sentences, vocab):\n",
    "    return [\n",
    "        [word if word in vocab else '<UNK>' for word in word_tokenize(sentence)]\n",
    "        for sentence in sentences\n",
    "    ]\n",
    "\n",
    "# Step 6: Apply the function to train, validation, and test sets\n",
    "train_tokens = replace_with_unk(train_set, vocab)\n",
    "val_tokens = replace_with_unk(val_set, vocab)\n",
    "test_tokens = replace_with_unk(test_set, vocab)\n",
    "\n",
    "# Check the length of `train_tokens` after replacement\n",
    "if len(train_tokens) == 0:\n",
    "    print(\"Error: `train_tokens` is empty after replacement. Check `replace_with_unk` function.\")\n",
    "else:\n",
    "    print(f\"Number of sentences in train_tokens after replacement: {len(train_tokens)}\")\n",
    "\n",
    "# Step 7: Print the first 5 tokenized sentences to check results\n",
    "print(\"First 5 tokenized training sentences with <UNK>:\")\n",
    "for i in range(min(5, len(train_tokens))):  # Handle cases where there are fewer than 5 sentences\n",
    "    print(train_tokens[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Build 4-Gram Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-gram model (Backoff) has 1319 entries.\n",
      "4-gram model (Interpolation) has 1319 entries.\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Function to build an n-gram model\n",
    "def build_ngram_model(tokenized_sentences, n=4):\n",
    "    \"\"\"\n",
    "    Build an n-gram model from tokenized sentences.\n",
    "    :param tokenized_sentences: List of tokenized sentences.\n",
    "    :param n: Order of the n-gram model.\n",
    "    :return: A dictionary of n-grams and their counts.\n",
    "    \"\"\"\n",
    "    ngram_counts = defaultdict(int)\n",
    "    for sentence in tokenized_sentences:\n",
    "        # padded_sentence = ['<s>'] * (n - 1) + sentence + ['</s>']\n",
    "        padded_sentence = ['<s>'] * (n - 1) + sentence \n",
    "        for i in range(len(padded_sentence) - n + 1):\n",
    "            ngram = tuple(padded_sentence[i:i + n])\n",
    "            ngram_counts[ngram] += 1\n",
    "    return ngram_counts\n",
    "\n",
    "# Build the n-gram models\n",
    "lm1 = build_ngram_model(train_tokens, n=4)  # Backoff model\n",
    "lm2 = build_ngram_model(train_tokens, n=4)  # Interpolation model\n",
    "\n",
    "print(\"4-gram model (Backoff) has\", len(lm1), \"entries.\")\n",
    "print(\"4-gram model (Interpolation) has\", len(lm2), \"entries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backoff_prob(model, ngram):\n",
    "    \"\"\"\n",
    "    Calculate the probability of an n-gram using a backoff model.\n",
    "    :param model: N-gram counts.\n",
    "    :param ngram: The n-gram tuple.\n",
    "    :return: Probability of the n-gram.\n",
    "    \"\"\"\n",
    "    for i in range(len(ngram), 0, -1):  # Back off through lower-order n-grams\n",
    "        sub_ngram = ngram[-i:]\n",
    "        if sub_ngram in model:\n",
    "            return model[sub_ngram] / sum(\n",
    "                count for ngram_key, count in model.items() if ngram_key[:-1] == sub_ngram[:-1]\n",
    "            )\n",
    "    return 1e-6  # Small probability for unseen n-grams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolated_prob(model, ngram, lambdas, k=1):\n",
    "    \"\"\"\n",
    "    Calculate the probability of an n-gram using an interpolation model.\n",
    "    :param model: N-gram counts.\n",
    "    :param ngram: The n-gram tuple.\n",
    "    :param lambdas: Weights for interpolation.\n",
    "    :param k: Smoothing factor.\n",
    "    :return: Interpolated probability.\n",
    "    \"\"\"\n",
    "    prob = 0.0\n",
    "    for i in range(1, len(ngram) + 1):  # Iterate over all sub-ngrams\n",
    "        sub_ngram = ngram[-i:]\n",
    "        sub_ngram_count = model.get(sub_ngram, 0)\n",
    "        context_count = sum(\n",
    "            count for ngram_key, count in model.items() if ngram_key[:-1] == sub_ngram[:-1]\n",
    "        )\n",
    "        prob += lambdas[i - 1] * ((sub_ngram_count + k) / (context_count + k))\n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of LM1 (Backoff): 712435.7693701141\n",
      "Perplexity of LM2 (Interpolation): 1.0132663254521892\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def calculate_perplexity(model, tokens, prob_func, n=4, lambdas=None, k=1):\n",
    "    \"\"\"\n",
    "    Calculate perplexity of a given language model on a test corpus.\n",
    "    :param model: Language model (Backoff or Interpolation).\n",
    "    :param tokens: Tokenized test set.\n",
    "    :param prob_func: Probability function (backoff_prob or interpolated_prob).\n",
    "    :param n: Order of the n-gram model.\n",
    "    :param lambdas: Weights for interpolation (if applicable).\n",
    "    :param k: Smoothing parameter (for interpolation).\n",
    "    \"\"\"\n",
    "    log_prob_sum = 0\n",
    "    word_count = 0\n",
    "\n",
    "    for sentence in tokens:\n",
    "        # padded_sentence = [\"<s>\"] * (n - 1) + sentence + [\"</s>\"]\n",
    "        padded_sentence = [\"<s>\"] * (n - 1) + sentence\n",
    "        for i in range(len(padded_sentence) - n + 1):\n",
    "            ngram = tuple(padded_sentence[i:i + n])\n",
    "            prob = prob_func(model, ngram, lambdas, k) if lambdas else prob_func(model, ngram)\n",
    "            log_prob_sum += math.log2(prob)\n",
    "            word_count += 1\n",
    "\n",
    "    return 2 ** (-log_prob_sum / word_count)\n",
    "\n",
    "# Hyperparameters for interpolation\n",
    "lambdas = [0.1, 0.3, 0.4, 0.2]\n",
    "k = 1\n",
    "\n",
    "# Calculate perplexity for Backoff and Interpolation models\n",
    "perplexity_lm1 = calculate_perplexity(lm1, test_tokens, prob_func=backoff_prob, n=4)\n",
    "perplexity_lm2 = calculate_perplexity(lm2, test_tokens, prob_func=interpolated_prob, n=4, lambdas=lambdas, k=k)\n",
    "\n",
    "print(f\"Perplexity of LM1 (Backoff): {perplexity_lm1}\")\n",
    "print(f\"Perplexity of LM2 (Interpolation): {perplexity_lm2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text by LM1 (Backoff): in terms of spatial extent , this makes it the largest urban agglomeration in recorded history prior to the industrial revolution , easily surpassing the nearest claim by the maya\n",
      "Generated Text by LM2 (Interpolation): over the ruins of angkor are located amid forests and farmland north of the great lake and south of the kulen hills , near modernday siem reap city , in\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_text(model, n=4, length=20, prob_func=None, lambdas=None, k=1):\n",
    "    \"\"\"\n",
    "    Generate text using a given language model.\n",
    "    :param model: Language model (Backoff or Interpolation).\n",
    "    :param n: Order of the n-gram model.\n",
    "    :param length: Number of tokens to generate.\n",
    "    :param prob_func: Probability function (backoff_prob or interpolated_prob).\n",
    "    :param lambdas: Weights for interpolation (if applicable).\n",
    "    :param k: Smoothing parameter (for interpolation).\n",
    "    :return: Generated text as a string.\n",
    "    \"\"\"\n",
    "    text = [\"<s>\"] * (n - 1)  # Start with padding symbols\n",
    "    for _ in range(length):\n",
    "        context = tuple(text[-(n - 1):])\n",
    "        candidates = [ngram for ngram in model if ngram[:-1] == context]\n",
    "        if candidates:\n",
    "            if lambdas:\n",
    "                weights = [prob_func(model, ngram, lambdas, k) for ngram in candidates]\n",
    "            else:\n",
    "                weights = [prob_func(model, ngram) for ngram in candidates]\n",
    "            chosen = random.choices(candidates, weights=weights)[0]\n",
    "            text.append(chosen[-1])\n",
    "\n",
    "\n",
    "    return \" \".join(text[n - 1:])  # Skip padding symbols in output\n",
    "\n",
    "# Generate text with both models\n",
    "generated_text_lm1 = generate_text(lm1, prob_func=backoff_prob, n=4, length=30)\n",
    "generated_text_lm2 = generate_text(lm2, prob_func=interpolated_prob, n=4, length=30, lambdas=lambdas, k=k)\n",
    "\n",
    "print(\"Generated Text by LM1 (Backoff):\", generated_text_lm1)\n",
    "print(\"Generated Text by LM2 (Interpolation):\", generated_text_lm2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
