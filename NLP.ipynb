{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load and Preprocess the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Remove citations like [1], [citation needed]\n",
    "    text = re.sub(r'\\[[^\\]]+\\]', '', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "    # Remove parenthetical content\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "    # Remove non-alphanumeric characters except periods\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s\\.]', '', text)\n",
    "    # Remove standalone numbers\n",
    "    text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Ensure proper sentence spacing (e.g., \"word.word\" -> \"word. word\")\n",
    "    text = re.sub(r'\\.([a-zA-Z])', r'. \\1', text)\n",
    "    return text.lower()  # Convert to lowercase\n",
    "\n",
    "# Load the raw corpus\n",
    "with open('./datasets/angkorwat.txt', 'r') as file:\n",
    "    raw_corpus = file.read()\n",
    "\n",
    "# Clean the corpus\n",
    "cleaned_corpus = clean_text(raw_corpus)\n",
    "\n",
    "# Save the cleaned corpus for inspection (optional)\n",
    "with open('./datasets/cleaned_angkorwat.txt', 'w') as file:\n",
    "    file.write(cleaned_corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Split the Corpus into Train, Validation, and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 71\n",
      "Training set: 49\n",
      "Validation set: 7\n",
      "Testing set: 15\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Split the text into sentences\n",
    "sentences = cleaned_corpus.split('.')\n",
    "# Remove empty sentences and trim whitespace\n",
    "sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "# Shuffle the sentences to avoid bias\n",
    "random.shuffle(sentences)\n",
    "\n",
    "# Calculate split indices\n",
    "train_split = int(0.7 * len(sentences))\n",
    "val_split = int(0.8 * len(sentences))\n",
    "\n",
    "# Create subsets\n",
    "train_set = sentences[:train_split]\n",
    "val_set = sentences[train_split:val_split]\n",
    "test_set = sentences[val_split:]\n",
    "\n",
    "# Print information about the splits\n",
    "print(f\"Total sentences: {len(sentences)}\")\n",
    "print(f\"Training set: {len(train_set)}\")\n",
    "print(f\"Validation set: {len(val_set)}\")\n",
    "print(f\"Testing set: {len(test_set)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Tokenize Sentences and Limit Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokenized sentences in train_set: 49\n",
      "Vocabulary size (top 20000 words): 568\n",
      "Number of sentences in train_tokens after replacement: 49\n",
      "First 5 tokenized training sentences with <UNK>:\n",
      "['the', 'alternate', 'name', 'yasodharapura', 'was', 'derived', 'from', 'the', 'name', 'of', 'the', 'foster', 'mother', 'of', 'lord', 'krishna', 'in', 'hinduism']\n",
      "['because', 'of', 'the', 'lowdensity', 'and', 'dispersed', 'nature', 'of', 'the', 'medieval', 'khmer', 'settlement', 'pattern', 'angkor', 'lacks', 'a', 'formal', 'boundary', 'and', 'its', 'extent', 'is', 'therefore', 'difficult', 'to', 'determine']\n",
      "['over', 'the', 'ruins', 'of', 'yasodharapura', 'jayavarman', 'constructed', 'the', 'walled', 'city', 'of', 'angkor', 'thom', 'as', 'well', 'as', 'its', 'geographic', 'and', 'spiritual', 'center', 'the', 'temple', 'known', 'as', 'the', 'bayon']\n",
      "['a', 'khmer', 'rebellion', 'against', 'siamese', 'authority', 'resulted', 'in', 'the', 'sacking', 'of', 'angkor', 'by', 'ayutthaya', 'causing', 'its', 'population', 'to', 'migrate', 'south', 'to', 'longvek']\n",
      "['the', 'angkorian', 'period', 'began', 'in', 'ad', 'when', 'the', 'khmer', 'hindu', 'monarch', 'jayavarman', 'ii', 'declared', 'himself', 'a', 'universal', 'monarch', 'and', 'godking', 'and', 'lasted', 'until', 'the', 'late', '14th', 'century', 'first', 'falling', 'under', 'ayutthayan', 'suzerainty', 'in']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# Step 1: Tokenize each sentence in the training set\n",
    "train_tokens = [word_tokenize(sentence) for sentence in train_set]\n",
    "\n",
    "# Check if tokenization is successful\n",
    "if len(train_tokens) == 0:\n",
    "    print(\"Error: `train_tokens` is empty. Check `train_set`.\")\n",
    "else:\n",
    "    print(f\"Number of tokenized sentences in train_set: {len(train_tokens)}\")\n",
    "\n",
    "# Step 2: Define the vocabulary size\n",
    "vocab_size = 20000\n",
    "\n",
    "# Step 3: Count word frequencies in the training set\n",
    "word_counts = Counter(word for sentence in train_tokens for word in sentence)\n",
    "\n",
    "# Step 4: Select the top `vocab_size` words as the vocabulary\n",
    "vocab = {word for word, _ in word_counts.most_common(vocab_size)}\n",
    "print(f\"Vocabulary size (top {vocab_size} words): {len(vocab)}\")\n",
    "\n",
    "# Step 5: Function to replace words not in the vocabulary with <UNK>\n",
    "def replace_with_unk(sentences, vocab):\n",
    "    return [\n",
    "        [word if word in vocab else '<UNK>' for word in word_tokenize(sentence)]\n",
    "        for sentence in sentences\n",
    "    ]\n",
    "\n",
    "# Step 6: Apply the function to train, validation, and test sets\n",
    "train_tokens = replace_with_unk(train_set, vocab)\n",
    "val_tokens = replace_with_unk(val_set, vocab)\n",
    "test_tokens = replace_with_unk(test_set, vocab)\n",
    "\n",
    "# Check the length of `train_tokens` after replacement\n",
    "if len(train_tokens) == 0:\n",
    "    print(\"Error: `train_tokens` is empty after replacement. Check `replace_with_unk` function.\")\n",
    "else:\n",
    "    print(f\"Number of sentences in train_tokens after replacement: {len(train_tokens)}\")\n",
    "\n",
    "# Step 7: Print the first 5 tokenized sentences to check results\n",
    "print(\"First 5 tokenized training sentences with <UNK>:\")\n",
    "for i in range(min(5, len(train_tokens))):  # Handle cases where there are fewer than 5 sentences\n",
    "    print(train_tokens[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Build 4-Gram Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import defaultdict\n",
    "\n",
    "# Function to build the n-gram model\n",
    "def build_ngram_model(tokens, n):\n",
    "    model = defaultdict(lambda: defaultdict(int))\n",
    "    for sentence in tokens:\n",
    "        for ngram in ngrams(sentence, n, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'):\n",
    "            prefix, token = ngram[:-1], ngram[-1]\n",
    "            model[prefix][token] += 1\n",
    "    return model\n",
    "\n",
    "# Build models using 4-grams\n",
    "lm1 = build_ngram_model(train_tokens, 4)  # Backoff model\n",
    "lm2 = build_ngram_model(train_tokens, 4)  # Interpolation model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Define Probability Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backoff Probability\n",
    "def backoff_prob(model, ngram):\n",
    "    prefix, token = ngram[:-1], ngram[-1]\n",
    "    if prefix in model and token in model[prefix]:\n",
    "        return model[prefix][token] / sum(model[prefix].values())\n",
    "    elif len(prefix) > 1:\n",
    "        return backoff_prob(model, prefix[1:] + (token,))\n",
    "    return 1e-10  # Small positive probability\n",
    "\n",
    "# Interpolation Probability\n",
    "def interpolated_prob(model, ngram, lambdas, k):\n",
    "    prob = 0\n",
    "    n = len(ngram)\n",
    "    for i in range(1, n + 1):\n",
    "        sub_ngram = ngram[-i:]\n",
    "        prefix, token = sub_ngram[:-1], sub_ngram[-1]\n",
    "        count = model[prefix][token] + k\n",
    "        total_count = sum(model[prefix].values()) + k * len(model[prefix])\n",
    "        prob += lambdas[i - 1] * (count / total_count)\n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Evaluate Models with Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of LM1 (Backoff): 4576457536.867848\n",
      "Perplexity of LM2 (Interpolation): 1.761733184278214\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Perplexity Calculation\n",
    "def calculate_perplexity(model, tokens, prob_func):\n",
    "    log_prob_sum = 0\n",
    "    word_count = 0\n",
    "    for sentence in tokens:\n",
    "        for ngram in ngrams(sentence, 4, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'):\n",
    "            prob = prob_func(model, ngram)\n",
    "            log_prob_sum += -math.log2(prob)\n",
    "            word_count += 1\n",
    "    return 2 ** (log_prob_sum / word_count)\n",
    "\n",
    "# Hyperparameters for interpolation\n",
    "lambdas = [0.1, 0.2, 0.3, 0.4]\n",
    "k = 1\n",
    "\n",
    "# Calculate perplexity for both models\n",
    "pp_lm1 = calculate_perplexity(lm1, test_tokens, lambda m, n: backoff_prob(m, n))\n",
    "pp_lm2 = calculate_perplexity(lm2, test_tokens, lambda m, n: interpolated_prob(m, n, lambdas, k))\n",
    "\n",
    "print(f\"Perplexity of LM1 (Backoff): {pp_lm1}\")\n",
    "print(f\"Perplexity of LM2 (Interpolation): {pp_lm2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text LM1: <s> <s> the alternate name yasodharapura was derived from the name of the foster mother of lord krishna in hinduism </s>\n",
      "Generated Text LM2: <s> <s> the alternate name yasodharapura was derived from the name of the foster mother of lord krishna in hinduism </s> </s>\n"
     ]
    }
   ],
   "source": [
    "# Text Generation\n",
    "def generate_text(model, start_tokens, max_length, prob_func):\n",
    "    text = start_tokens\n",
    "    for _ in range(max_length - len(start_tokens)):\n",
    "        prefix = tuple(text[-3:])\n",
    "        if prefix not in model:\n",
    "            break\n",
    "        next_word = max(model[prefix], key=lambda word: prob_func(model, prefix + (word,)))\n",
    "        text.append(next_word)\n",
    "        if next_word == '</s>':\n",
    "            break\n",
    "    return ' '.join(text)\n",
    "\n",
    "# Generate text for both models\n",
    "start = ['<s>', '<s>', 'the']\n",
    "print(\"Generated Text LM1:\", generate_text(lm1, start, 100, backoff_prob))\n",
    "print(\"Generated Text LM2:\", generate_text(lm2, start, 100, lambda m, n: interpolated_prob(m, n, lambdas, k)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
