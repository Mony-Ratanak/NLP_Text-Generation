{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP mini-project Group3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Library Installation </h3>\n",
    "<ul>\n",
    "    <li>pip install wikipedia-api</li>\n",
    "    <li>pip install nltk</li>\n",
    "    <li>nltk.download('punkt')  # For sentence tokenization</li>\n",
    "    <li>nltk.download('stopwords')  # For removing stop words</li>\n",
    "    <li>nltk.download('wordnet')  # For lexical semantics</li>\n",
    "    <li>nltk.download('punkt') #For sentence tokenizer</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. Data Collection</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 'Khmer Empire' has been saved to ./datasets\\khmer_empire.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wikipediaapi\n",
    "\n",
    "# Set up the Wikipedia API with a valid user-agent\n",
    "wiki = wikipediaapi.Wikipedia(\n",
    "    language=\"en\", \n",
    "    user_agent=\"YourAppName/1.0 (your-email@example.com)\"\n",
    ")\n",
    "\n",
    "# Specify the topic and folder path\n",
    "topic = \"Khmer Empire\"\n",
    "output_folder = \"./datasets\"\n",
    "output_file = os.path.join(output_folder, \"khmer_empire.txt\")\n",
    "\n",
    "# Fetch the article\n",
    "article = wiki.page(topic)\n",
    "\n",
    "if article.exists():\n",
    "    # Ensure the folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Write the article text to a file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(article.text)\n",
    "    print(f\"Article '{topic}' has been saved to {output_file}\")\n",
    "else:\n",
    "    print(f\"The article '{topic}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. Data Preprocessing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8871238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaned Article Khmer Empire has been saved to ./datasets/cleaned_khmer_empire.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Remove citations like [1], [citation needed]\n",
    "    text = re.sub(r'\\[[^\\]]+\\]', '', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "    # Remove parenthetical content\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "    # Allow commas and periods while removing other punctuation\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,]', '', text)\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Ensure proper sentence spacing (e.g., \"word.word\" -> \"word. word\")\n",
    "    text = re.sub(r'\\.([a-zA-Z])', r'. \\1', text)\n",
    "    return text.lower()  # Convert to lowercase\n",
    "\n",
    "# Load the raw corpus\n",
    "# Load the raw corpus\n",
    "with open('./datasets/khmer_empire.txt', 'r', encoding='utf-8') as file:\n",
    "    raw_corpus = file.read()\n",
    "\n",
    "\n",
    "# Clean the corpus\n",
    "cleaned_corpus = clean_text(raw_corpus)\n",
    "\n",
    "# Save the cleaned corpus for inspection (optional)\n",
    "with open('./datasets/cleaned_khmer_empire.txt', 'w') as file:\n",
    "    file.write(cleaned_corpus)\n",
    "print(f\"After cleaned Article Khmer Empire has been saved to ./datasets/cleaned_khmer_empire.txt\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3. Split the Corpus into training (70%), validation (10%) and testing (20%)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 356\n",
      "Training set: 249\n",
      "Validation set: 35\n",
      "Testing set: 72\n",
      "Datasets saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "# Split the text into sentences\n",
    "sentences = cleaned_corpus.split('.')\n",
    "# Remove empty sentences and trim whitespace\n",
    "sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "# Shuffle the sentences to avoid bias\n",
    "random.shuffle(sentences)\n",
    "\n",
    "# Calculate split indices\n",
    "train_split = int(0.7 * len(sentences))\n",
    "val_split = int(0.1 * len(sentences)) + train_split\n",
    "\n",
    "# Create subsets\n",
    "train_set = sentences[:train_split]\n",
    "val_set = sentences[train_split:val_split]\n",
    "test_set = sentences[val_split:]\n",
    "\n",
    "# Print information about the splits\n",
    "print(f\"Total sentences: {len(sentences)}\")\n",
    "print(f\"Training set: {len(train_set)}\")\n",
    "print(f\"Validation set: {len(val_set)}\")\n",
    "print(f\"Testing set: {len(test_set)}\")\n",
    "\n",
    "# Ensure the output directory exists\n",
    "output_dir = './datasets'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the split datasets\n",
    "train_file = os.path.join(output_dir, 'train_set.txt')\n",
    "val_file = os.path.join(output_dir, 'val_set.txt')\n",
    "test_file = os.path.join(output_dir, 'test_set.txt')\n",
    "\n",
    "# Save the data to files\n",
    "with open(train_file, 'w') as f:\n",
    "    f.write('\\n'.join(train_set))\n",
    "\n",
    "with open(val_file, 'w') as f:\n",
    "    f.write('\\n'.join(val_set))\n",
    "\n",
    "with open(test_file, 'w') as f:\n",
    "    f.write('\\n'.join(test_set))\n",
    "\n",
    "print(\"Datasets saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4 .Tokenize Sentences and Limit Vocabulary</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokenized sentences in train_set: 249\n",
      "Vocabulary size (top 20000 words): 1520\n",
      "Number of sentences in train_tokens after replacement: 249\n",
      "First 5 tokenized training sentences with <UNK>:\n",
      "['from', 'the', 'fourteenth', 'century', 'onward', ',', 'ayutthaya', 'became', 'kambujas', 'rival']\n",
      "['that', 'created', 'rain', 'runoff', 'carrying', 'sediment', 'to', 'the', 'canal', 'network']\n",
      "['the', 'citys', 'central', 'temple', 'was', 'built', 'on', 'phnom', 'bakheng', ',', 'a', 'hill', 'which', 'rises', 'around', '60', 'm', 'above', 'the', 'plain', 'on', 'which', 'angkor', 'sits']\n",
      "['behind', 'them', 'comes', 'the', 'sovereign', ',', 'standing', 'on', 'an', 'elephant', ',', 'holding', 'his', 'sacred', 'sword', 'in', 'his', 'hand']\n",
      "['arab', 'writers', 'of', 'the', '9th', 'and', '10th', 'century', 'hardly', 'mention', 'the', 'region', 'for', 'anything', 'other', 'than', 'its', 'perceived', 'backwardness', ',', 'but', 'they', 'considered', 'the', 'king', 'of', 'alhind', 'one', 'of', 'the', 'four', 'great', 'kings', 'in', 'the', 'world']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# Step 1: Tokenize each sentence in the training set\n",
    "train_tokens = [word_tokenize(sentence) for sentence in train_set]\n",
    "\n",
    "# Check if tokenization is successful\n",
    "if len(train_tokens) == 0:\n",
    "    print(\"Error: `train_tokens` is empty. Check `train_set`.\")\n",
    "else:\n",
    "    print(f\"Number of tokenized sentences in train_set: {len(train_tokens)}\")\n",
    "\n",
    "# Step 2: Define the vocabulary size\n",
    "vocab_size = 20000\n",
    "\n",
    "# Step 3: Count word frequencies in the training set\n",
    "word_counts = Counter(word for sentence in train_tokens for word in sentence)\n",
    "\n",
    "# Step 4: Select the top `vocab_size` words as the vocabulary\n",
    "vocab = {word for word, _ in word_counts.most_common(vocab_size)}\n",
    "print(f\"Vocabulary size (top {vocab_size} words): {len(vocab)}\")\n",
    "\n",
    "# Step 5: Function to replace words not in the vocabulary with <UNK>\n",
    "def replace_with_unk(sentences, vocab):\n",
    "    return [\n",
    "        [word if word in vocab else '<UNK>' for word in word_tokenize(sentence)]\n",
    "        for sentence in sentences\n",
    "    ]\n",
    "\n",
    "# Step 6: Apply the function to train, validation, and test sets\n",
    "train_tokens = replace_with_unk(train_set, vocab)\n",
    "val_tokens = replace_with_unk(val_set, vocab)\n",
    "test_tokens = replace_with_unk(test_set, vocab)\n",
    "\n",
    "# Check the length of `train_tokens` after replacement\n",
    "if len(train_tokens) == 0:\n",
    "    print(\"Error: `train_tokens` is empty after replacement. Check `replace_with_unk` function.\")\n",
    "else:\n",
    "    print(f\"Number of sentences in train_tokens after replacement: {len(train_tokens)}\")\n",
    "\n",
    "# Step 7: Print the first 5 tokenized sentences to check results\n",
    "print(\"First 5 tokenized training sentences with <UNK>:\")\n",
    "for i in range(min(5, len(train_tokens))):  # Handle cases where there are fewer than 5 sentences\n",
    "    print(train_tokens[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>5. Build 4-Gram Models</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-gram model (Backoff) has 4496 entries.\n",
      "4-gram model (Interpolation) has 4496 entries.\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Function to build an n-gram model\n",
    "def build_ngram_model(tokenized_sentences, n=4):\n",
    "    ngram_counts = defaultdict(int)\n",
    "    for sentence in tokenized_sentences:\n",
    "        padded_sentence = ['<s>'] * (n - 1) + sentence \n",
    "        for i in range(len(padded_sentence) - n + 1):\n",
    "            ngram = tuple(padded_sentence[i:i + n])\n",
    "            ngram_counts[ngram] += 1\n",
    "    return ngram_counts\n",
    "\n",
    "# Build the n-gram models\n",
    "lm1 = build_ngram_model(train_tokens, n=4)  # Backoff model\n",
    "lm2 = build_ngram_model(train_tokens, n=4)  # Interpolation model\n",
    "\n",
    "print(\"4-gram model (Backoff) has\", len(lm1), \"entries.\")\n",
    "print(\"4-gram model (Interpolation) has\", len(lm2), \"entries.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>6. Calculate the probability of n-gram</h3>\n",
    "<h4><li>Using Backoff model</li></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backoff_prob(model, ngram):\n",
    "\n",
    "    for i in range(len(ngram), 0, -1):  # Back off through lower-order n-grams\n",
    "        sub_ngram = ngram[-i:]\n",
    "        if sub_ngram in model:\n",
    "            return model[sub_ngram] / sum(\n",
    "                count for ngram_key, count in model.items() if ngram_key[:-1] == sub_ngram[:-1]\n",
    "            )\n",
    "    return 1e-6  # Small probability for unseen n-grams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><li>Using Interpolated model</li></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolated_prob(model, ngram, lambdas, k=1):\n",
    "\n",
    "    prob = 0.0\n",
    "    for i in range(1, len(ngram) + 1):  # Iterate over all sub-ngrams\n",
    "        sub_ngram = ngram[-i:]\n",
    "        sub_ngram_count = model.get(sub_ngram, 0)\n",
    "        context_count = sum(\n",
    "            count for ngram_key, count in model.items() if ngram_key[:-1] == sub_ngram[:-1]\n",
    "        )\n",
    "        prob += lambdas[i - 1] * ((sub_ngram_count + k) / (context_count + k))\n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>7. Calulation Perplexity for each models</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of LM1 (Backoff): 438876.96392070514\n",
      "Perplexity of LM2 (Interpolation): 1.0230946327385118\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def calculate_perplexity(model, tokens, prob_func, n=4, lambdas=None, k=1):\n",
    "    log_prob_sum = 0\n",
    "    word_count = 0\n",
    "\n",
    "    for sentence in tokens:\n",
    "        padded_sentence = [\"<s>\"] * (n - 1) + sentence\n",
    "        for i in range(len(padded_sentence) - n + 1):\n",
    "            ngram = tuple(padded_sentence[i:i + n])\n",
    "            prob = prob_func(model, ngram, lambdas, k) if lambdas else prob_func(model, ngram)\n",
    "            log_prob_sum += math.log2(prob)\n",
    "            word_count += 1\n",
    "\n",
    "    return 2 ** (-log_prob_sum / word_count)\n",
    "\n",
    "# Hyperparameters for interpolation\n",
    "lambdas = [0.1, 0.3, 0.4, 0.2]\n",
    "k = 1\n",
    "\n",
    "# Calculate perplexity for Backoff and Interpolation models\n",
    "perplexity_lm1 = calculate_perplexity(lm1, test_tokens, prob_func=backoff_prob, n=4)\n",
    "perplexity_lm2 = calculate_perplexity(lm2, test_tokens, prob_func=interpolated_prob, n=4, lambdas=lambdas, k=k)\n",
    "\n",
    "print(f\"Perplexity of LM1 (Backoff): {perplexity_lm1}\")\n",
    "print(f\"Perplexity of LM2 (Interpolation): {perplexity_lm2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>8. Text Generation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text by LM1 (Backoff): historians have proposed different causes for the decline the religious conversion from vishnuiteshivaite hinduism to theravada buddhism that affected social and political systems , incessant internal power struggles among khmer\n",
      "\n",
      "Generated Text by LM2 (Interpolation): art and architecture reached their aesthetic and technical peak with the construction of the majestic temple angkor wat\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_text(model, n=4, length=20, prob_func=None, lambdas=None, k=1):\n",
    "    text = [\"<s>\"] * (n - 1)  # Start with padding symbols\n",
    "    for _ in range(length):\n",
    "        context = tuple(text[-(n - 1):])\n",
    "        candidates = [ngram for ngram in model if ngram[:-1] == context]\n",
    "        if candidates:\n",
    "            if lambdas:\n",
    "                weights = [prob_func(model, ngram, lambdas, k) for ngram in candidates]\n",
    "            else:\n",
    "                weights = [prob_func(model, ngram) for ngram in candidates]\n",
    "            chosen = random.choices(candidates, weights=weights)[0]\n",
    "            text.append(chosen[-1])\n",
    "\n",
    "\n",
    "    return \" \".join(text[n - 1:])  # Skip padding symbols in output\n",
    "\n",
    "# Generate text with both models\n",
    "generated_text_lm1 = generate_text(lm1, prob_func=backoff_prob, n=4, length=30)\n",
    "generated_text_lm2 = generate_text(lm2, prob_func=interpolated_prob, n=4, length=30, lambdas=lambdas, k=k)\n",
    "\n",
    "print(\"\\nGenerated Text by LM1 (Backoff):\", generated_text_lm1)\n",
    "print(\"\\nGenerated Text by LM2 (Interpolation):\", generated_text_lm2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
